import torch
import torch.nn as nn


class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=500):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:, :x.size(1)]


class TransformerCTC(nn.Module):
    def __init__(self, input_dim=244, model_dim=256, num_classes=13, num_heads=4, num_layers=4, dropout=0.1):
        super().__init__()

        print(f"ğŸ¯ Transformer CTC ëª¨ë¸ ì´ˆê¸°í™”")
        print(f"ğŸ“Š ì…ë ¥ ì°¨ì›: {input_dim}")
        print(f"ğŸ·ï¸ í´ë˜ìŠ¤ ìˆ˜: {num_classes}")
        print(f"ğŸ§  ëª¨ë¸ ì°¨ì›: {model_dim}")
        print(f"ğŸ‘ï¸ í—¤ë“œ ìˆ˜: {num_heads}")
        print(f"ğŸ—ï¸ ë ˆì´ì–´ ìˆ˜: {num_layers}")

        # 244ì°¨ì› â†’ model_dim ë³€í™˜
        self.input_fc = nn.Linear(input_dim, model_dim)
        self.pos_encoder = PositionalEncoding(model_dim)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=model_dim,
            nhead=num_heads,
            dropout=dropout,
            batch_first=True,
            activation='gelu'  # ë” ë‚˜ì€ í™œì„±í™” í•¨ìˆ˜
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # í”„ë ˆì„ë³„ í´ë˜ìŠ¤ ì˜ˆì¸¡ (CTCìš©)
        self.classifier = nn.Linear(model_dim, num_classes + 1)  # +1ì€ CTC blank label

        # ë“œë¡­ì•„ì›ƒ ì¶”ê°€
        self.dropout = nn.Dropout(dropout)

        # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        print(f"ğŸ“ˆ ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
        print(f"ğŸ“ í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,}")

    def forward(self, x):
        """
        Args:
            x: (B, T, 244) - ë°°ì¹˜, ì‹œí€€ìŠ¤, 244ì°¨ì› íŠ¹ì§•

        Returns:
            logits: (B, T, num_classes+1) - CTCìš© ì¶œë ¥
        """
        # ì…ë ¥ ì°¨ì› í™•ì¸
        assert x.shape[-1] == 244, f"ì…ë ¥ ì°¨ì›ì´ 244ê°€ ì•„ë‹˜: {x.shape[-1]}"

        # 244ì°¨ì› â†’ model_dim ë³€í™˜
        x = self.input_fc(x)  # (B, T, model_dim)
        x = self.dropout(x)  # ë“œë¡­ì•„ì›ƒ

        # Positional Encoding ì¶”ê°€
        x = self.pos_encoder(x)  # (B, T, model_dim)

        # Transformer Encoder
        x = self.transformer_encoder(x)  # (B, T, model_dim)

        # ë¶„ë¥˜ê¸°
        logits = self.classifier(x)  # (B, T, num_classes+1)

        return logits

    def get_model_info(self):
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            'input_dim': 244,
            'num_classes': 13,
            'model_dim': self.input_fc.out_features,
            'num_heads': self.transformer_encoder.layers[0].self_attn.num_heads,
            'num_layers': len(self.transformer_encoder.layers),
            'total_params': sum(p.numel() for p in self.parameters()),
            'trainable_params': sum(p.numel() for p in self.parameters() if p.requires_grad)
        }


# ëª¨ë¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_model():
    """ëª¨ë¸ ì‘ë™ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("=" * 30)

    # ëª¨ë¸ ìƒì„±
    model = TransformerCTC(
        input_dim=244,
        model_dim=256,
        num_classes=13,
        num_heads=8,
        num_layers=6,
        dropout=0.1
    )

    # ë”ë¯¸ ë°ì´í„° ìƒì„± (batch_size=4, seq_len=30, features=244)
    dummy_input = torch.randn(4, 30, 244)

    print(f"\nğŸ“¥ ì…ë ¥ í˜•íƒœ: {dummy_input.shape}")

    # Forward pass
    with torch.no_grad():
        output = model(dummy_input)

    print(f"ğŸ“¤ ì¶œë ¥ í˜•íƒœ: {output.shape}")
    print(f"âœ… ì˜ˆìƒ í˜•íƒœ: (4, 30, 14) - 13í´ë˜ìŠ¤ + 1blank")

    if output.shape == (4, 30, 14):
        print("ğŸ‰ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
    else:
        print("âŒ ëª¨ë¸ ì¶œë ¥ í˜•íƒœ ì˜¤ë¥˜")

    return model


if __name__ == "__main__":
    model = test_model()
    print(f"\nğŸ“Š ëª¨ë¸ ì •ë³´:")
    info = model.get_model_info()
    for key, value in info.items():
        print(f"  {key}: {value:,}" if isinstance(value, int) else f"  {key}: {value}")