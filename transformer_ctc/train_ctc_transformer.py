import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
import numpy as np

from transformer_ctc_model import TransformerCTC

# === í•˜ì´í¼íŒŒë¼ë¯¸í„° ===
BATCH_SIZE = 16
EPOCHS = 30
LR = 0.0001
NUM_CLASS = 13  # ì‹¤ì œ í´ë˜ìŠ¤ ê°œìˆ˜ (blank ì œì™¸) - 13ê°œë¡œ ë³€ê²½
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print("ğŸ¯ 244ì°¨ì› CTC Transformer í•™ìŠµ ì‹œì‘")
print("=" * 50)
print("ğŸš€ Device ì„¤ì •:", DEVICE)

if DEVICE.type == "cuda":
    print("âœ… GPU ì‚¬ìš© ì¤‘:", torch.cuda.get_device_name(DEVICE))
    print("   í• ë‹¹ëœ ë©”ëª¨ë¦¬:", round(torch.cuda.memory_allocated(DEVICE) / 1024 ** 2, 2), "MB")
else:
    print("âš ï¸ GPUë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  CPUë§Œ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤.")

# === ë°ì´í„° ë¡œë“œ ===
print("\nğŸ“‚ ë°ì´í„° ë¡œë”©...")
X = np.load("X_unified_244_aug.npy")  # (N, T, 244) - íŒŒì¼ëª… ë³€ê²½
y = np.load("y_unified_244_aug.npy")  # (N,) - íŒŒì¼ëª… ë³€ê²½

print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: X={X.shape}, y={y.shape}")
print(f"ğŸ“Š ì‹œí€€ìŠ¤ ê¸¸ì´: {X.shape[1]}")
print(f"ğŸ“ íŠ¹ì§• ì°¨ì›: {X.shape[2]}")

# í´ë˜ìŠ¤ë³„ ë¶„í¬ í™•ì¸
unique, counts = np.unique(y, return_counts=True)
print(f"\nğŸ“ˆ í´ë˜ìŠ¤ë³„ ë°ì´í„° ë¶„í¬:")
for class_id, count in zip(unique, counts):
    print(f"  í´ë˜ìŠ¤ {class_id}: {count}ê°œ")

seq_len = X.shape[1]

# === CTCìš© ë¼ë²¨ ì²˜ë¦¬ ===
targets = [torch.tensor([label], dtype=torch.long) for label in y]  # ê° ìƒ˜í”Œë‹¹ 1ê°œì˜ í´ë˜ìŠ¤
target_lengths = torch.tensor([len(t) for t in targets], dtype=torch.long)

X = torch.tensor(X, dtype=torch.float32)
y = torch.cat(targets)  # (N,)

# === ë°ì´í„° ë¶„í•  ===
print("\nğŸ”„ ë°ì´í„° ë¶„í•  (Train:Val = 8:2)")
X_train, X_val, y_train, y_val, tgt_len_train, tgt_len_val = train_test_split(
    X, y, target_lengths, test_size=0.2, stratify=y, random_state=42
)

print(f"ğŸ“Š í•™ìŠµ ë°ì´í„°: {len(X_train)}ê°œ")
print(f"ğŸ“Š ê²€ì¦ ë°ì´í„°: {len(X_val)}ê°œ")

train_dataset = TensorDataset(X_train, y_train, tgt_len_train)
val_dataset = TensorDataset(X_val, y_val, tgt_len_val)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

print(f"ğŸ”¢ ë°°ì¹˜ ìˆ˜ - í•™ìŠµ: {len(train_loader)}, ê²€ì¦: {len(val_loader)}")

# === ëª¨ë¸ ì´ˆê¸°í™” ===
print("\nğŸ§  ëª¨ë¸ ì´ˆê¸°í™”...")
model = TransformerCTC(
    input_dim=244,  # 244ì°¨ì›ìœ¼ë¡œ ë³€ê²½
    num_classes=NUM_CLASS,  # 13ê°œ í´ë˜ìŠ¤
    model_dim=256,
    num_heads=8,
    num_layers=6,
    dropout=0.1
).to(DEVICE)

criterion = nn.CTCLoss(blank=NUM_CLASS, zero_infinity=True)
optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)

# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì¶”ê°€
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=5
)

best_val_loss = float("inf")
best_val_acc = 0.0

print(f"\nğŸ“‹ í•™ìŠµ ì„¤ì •:")
print(f"  ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}")
print(f"  ì—í¬í¬: {EPOCHS}")
print(f"  í•™ìŠµë¥ : {LR}")
print(f"  í´ë˜ìŠ¤ ìˆ˜: {NUM_CLASS}")


# === Greedy decoding í•¨ìˆ˜ ===
def greedy_decode(logits, blank=NUM_CLASS):
    """
    logits: (B, T, C+1)
    return: (B,) ì˜ˆì¸¡ëœ class
    """
    pred = logits.argmax(dim=2)  # (B, T)
    decoded = []
    for p in pred:
        # CTC ê·œì¹™: blank ì œê±°, ì—°ì†ëœ ê°™ì€ ë¼ë²¨ í•©ì¹˜ê¸°
        prev = blank
        seq = []
        for label in p:
            label = label.item()
            if label != blank and label != prev:
                seq.append(label)
            prev = label
        # ì´ë²ˆ ê²½ìš°ëŠ” ë¼ë²¨ í•˜ë‚˜ë§Œ ë‚¨ê¸°ë¯€ë¡œ ì²«ë²ˆì§¸ ë¼ë²¨ ì‚¬ìš©
        decoded.append(seq[0] if len(seq) > 0 else blank)
    return decoded


# === í•™ìŠµ ë£¨í”„ ===
print(f"\nğŸš€ í•™ìŠµ ì‹œì‘!")
print("=" * 70)

for epoch in range(1, EPOCHS + 1):
    # === í•™ìŠµ ë‹¨ê³„ ===
    model.train()
    train_loss, train_correct = 0.0, 0

    for batch_idx, (x_batch, y_batch, tgt_lens) in enumerate(train_loader):
        x_batch, y_batch, tgt_lens = x_batch.to(DEVICE), y_batch.to(DEVICE), tgt_lens.to(DEVICE)

        optimizer.zero_grad()
        logits = model(x_batch)  # (B, T, C+1)
        log_probs = nn.functional.log_softmax(logits, dim=2)
        input_lens = torch.full((logits.size(0),), logits.size(1), dtype=torch.long).to(DEVICE)

        loss = criterion(log_probs.permute(1, 0, 2), y_batch, input_lens, tgt_lens)

        # gradient clipping ì¶”ê°€
        if not torch.isnan(loss) and not torch.isinf(loss):
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            train_loss += loss.item()

        # === ì •í™•ë„ ê³„ì‚° ===
        with torch.no_grad():
            preds = greedy_decode(log_probs, blank=NUM_CLASS)
            train_correct += (torch.tensor(preds).to(DEVICE) == y_batch).sum().item()

    # === ê²€ì¦ ë‹¨ê³„ ===
    model.eval()
    val_loss, val_correct = 0.0, 0
    with torch.no_grad():
        for x_batch, y_batch, tgt_lens in val_loader:
            x_batch, y_batch, tgt_lens = x_batch.to(DEVICE), y_batch.to(DEVICE), tgt_lens.to(DEVICE)

            logits = model(x_batch)
            log_probs = nn.functional.log_softmax(logits, dim=2)
            input_lens = torch.full((logits.size(0),), logits.size(1), dtype=torch.long).to(DEVICE)

            loss = criterion(log_probs.permute(1, 0, 2), y_batch, input_lens, tgt_lens)
            if not torch.isnan(loss) and not torch.isinf(loss):
                val_loss += loss.item()

            # === ì •í™•ë„ ê³„ì‚° ===
            preds = greedy_decode(log_probs, blank=NUM_CLASS)
            val_correct += (torch.tensor(preds).to(DEVICE) == y_batch).sum().item()

    # === ë©”íŠ¸ë¦­ ê³„ì‚° ===
    avg_train_loss = train_loss / len(train_loader) if train_loss > 0 else float('inf')
    avg_val_loss = val_loss / len(val_loader) if val_loss > 0 else float('inf')
    train_acc = train_correct / len(train_dataset) * 100
    val_acc = val_correct / len(val_dataset) * 100

    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
    scheduler.step(avg_val_loss)

    print(f"[Epoch {epoch:2d}/{EPOCHS}] "
          f"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:5.2f}% | "
          f"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:5.2f}%")

    # === ìµœê³  ëª¨ë¸ ì €ì¥ ===
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        best_val_acc = val_acc
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'best_val_loss': best_val_loss,
            'best_val_acc': best_val_acc,
            'model_config': {
                'input_dim': 244,
                'num_classes': NUM_CLASS,
                'model_dim': 256,
                'num_heads': 8,
                'num_layers': 6
            }
        }, "best_ctc_transformer_244.pt")
        print(f"âœ… Best model ì—…ë°ì´íŠ¸! (Val Acc: {val_acc:.2f}%)")

    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    if DEVICE.type == "cuda":
        torch.cuda.empty_cache()

print("\nğŸ‰ CTC Transformer í•™ìŠµ ì™„ë£Œ!")
print("=" * 50)
print(f"ğŸ† ìµœê³  ê²€ì¦ ì •í™•ë„: {best_val_acc:.2f}%")
print(f"ğŸ“„ ëª¨ë¸ ì €ì¥ë¨: best_ctc_transformer_244.pt")

# === ìµœì¢… ëª¨ë¸ ì •ë³´ ì¶œë ¥ ===
print(f"\nğŸ“Š ìµœì¢… ëª¨ë¸ ì •ë³´:")
model_info = model.get_model_info()
for key, value in model_info.items():
    if isinstance(value, int):
        print(f"  {key}: {value:,}")
    else:
        print(f"  {key}: {value}")